{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "from segmentation_models import PSPNet\n",
    "from segmentation_models import FPN\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models import Linknet\n",
    "from segmentation_models.segmentation_models.backbones import get_preprocessing\n",
    "\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "import spacexyz\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = \"../train_images/\"\n",
    "train_label_path = \"../train_labels/\"\n",
    "val_image_path = \"../val_images\"\n",
    "val_label_path = \"../val_labels\"\n",
    "\n",
    "n_classes = 1+6\n",
    "\n",
    "input_size = (512, 512, 3)\n",
    "output_size = (512, 512)\n",
    "\n",
    "# Unet, PSPNet, FPN, Linknet\n",
    "model_name = 'Unet'\n",
    "\n",
    "# vgg16, vgg19, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50, resnext101, densenet121, densenet169, densenet201, inceptionv3, inceptionresnetv2\n",
    "backbone_name = 'resnet34'\n",
    "\n",
    "# number of 10 epoches between saving models\n",
    "n_save = 5\n",
    "epc_num = 10\n",
    "\n",
    "# TODO to resize back the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "############# Read in files ########################\n",
    "####################################################\n",
    "\n",
    "train_images = spacexyz.path2filelist(train_image_path)\n",
    "train_labels = spacexyz.path2filelist(train_label_path)\n",
    "val_images = spacexyz.path2filelist(val_image_path)\n",
    "val_labels = spacexyz.path2filelist(val_label_path)\n",
    "\n",
    "assert(len(train_images) == len(train_labels))\n",
    "assert(len(val_images) == len(val_labels))\n",
    "\n",
    "n_training = len(train_images)\n",
    "n_val = len(val_images)\n",
    "print(\"Number of training images: \", n_training)\n",
    "print(\"Number of validation or testing images: \", n_val)\n",
    "\n",
    "# initialize data\n",
    "X_train = np.zeros([n_training, *input_size]).astype(np.uint8)\n",
    "y_train = np.zeros([n_training, *output_size]).astype(np.uint8)\n",
    "\n",
    "X_val = np.zeros([n_val, *input_size]).astype(np.uint8)\n",
    "y_val = np.zeros([n_val, *output_size]).astype(np.uint8)\n",
    "\n",
    "\n",
    "####################################################\n",
    "############# Read in training dataset #############\n",
    "####################################################\n",
    "\n",
    "train_size = np.zeros([n_training, 2])\n",
    "val_size = np.zeros([n_val, 2])\n",
    "\n",
    "print(\"reading in \", n_training, \" training samples...\")\n",
    "for i in range(n_training):\n",
    "    print(i, end='.')\n",
    "    t_image = cv2.imread(join(train_image_path, train_images[i]))\n",
    "    t_label = cv2.imread(join(train_label_path, train_labels[i]))\n",
    "    print(t_image.shape[:2])\n",
    "    train_size[i,:] = t_image.shape[:2]\n",
    "    X_train[i,:,:,:] = cv2.resize(t_image, input_size[:2])\n",
    "    y_train[i,:,:] = cv2.resize(t_label[:,:,0], output_size[:2], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=n_classes, dtype='float32')\n",
    "\n",
    "\n",
    "####################################################\n",
    "############# Read in validation dataset ###########\n",
    "####################################################\n",
    "\n",
    "print(\"reading in \", n_val, \" eval samples...\")\n",
    "for i in range(n_val):\n",
    "    print(i,end= '.')\n",
    "    v_image = cv2.imread(join(val_image_path, val_images[i]))\n",
    "    v_label = cv2.imread(join(val_label_path, val_labels[i]))\n",
    "    val_size[i,:] = v_image.shape[:2]\n",
    "    X_val[i,:,:,:] = cv2.resize(v_image, input_size[:2])\n",
    "    y_val[i,:,:] = cv2.resize(v_label[:,:,0], output_size[:2], interpolation=cv2.INTER_NEAREST)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=n_classes, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "############# Preprocess data ######################\n",
    "####################################################\n",
    "\n",
    "preprocessing_fn = get_preprocessing('resnet34')\n",
    "x = preprocessing_fn(X_train)\n",
    "\n",
    "## callback function to evaluate test data at the end of each training epoch\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "\n",
    "####################################################\n",
    "############# Set model parameters #################\n",
    "####################################################\n",
    "\n",
    "if model_name == 'Unet':\n",
    "    model = Unet(backbone_name=backbone_name, classes=n_classes, activation='softmax')\n",
    "elif model_name == 'PSPNet':\n",
    "    model = PSPNet(backbone_name=backbone_name, classes=n_classes, activation='softmax')\n",
    "elif model_name == 'FPN':\n",
    "    model = FPN(backbone_name=backbone_name, classes=n_classes, activation='softmax')\n",
    "elif model_name == 'Linknet':\n",
    "    model = Linknet(backbone_name=backbone_name, classes=n_classes, activation='softmax')\n",
    "else:\n",
    "    print('Please provide the right model name')\n",
    "\n",
    "model.compile('Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy']) \n",
    "\n",
    "\n",
    "####################################################\n",
    "############# Training model #######################\n",
    "####################################################\n",
    "\n",
    "for i in range(n_save):\n",
    "    print('==============================')\n",
    "    print('in iteration: ', i+1)\n",
    "    print('==============================')\n",
    "    \n",
    "    model.fit(x, y_train,  validation_data=(X_val, y_val), callbacks=[TestCallback((X_val, y_val))], batch_size=1, epochs=epc_num, verbose=True)\n",
    "    model_name = 'Unet_noaug_epoch_'+str(epc_num*(i+1))+'.h5'\n",
    "    \n",
    "    print('==============================')\n",
    "    print('saving model after epoch ', (i+1)*epc_num)\n",
    "    print('==============================')\n",
    "    \n",
    "#     save_name = model_name+\"_noaug_epoch_\"+str(10*(i+1))+\".h5\"\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./Unet_epoch_90.h5_noaug_epoch_90.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_val, batch_size=None, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to original\n",
    "\n",
    "k = 12\n",
    "p = pred[k,:,:]\n",
    "p_original = skimage.transform.resize(p, val_size[k,:], order=0, mode='reflect', cval=0)\n",
    "label = spacexyz.onehot2ind(p_original)\n",
    "segmap = label.astype(np.int32)\n",
    "segmap = ia.SegmentationMapOnImage(segmap, shape=val_size[k,:], nb_classes=n_classes)\n",
    "X_or = cv2.imread(join(train_image_path, train_images[k]))\n",
    "plt.imshow(segmap.draw_on_image(X_or))\n",
    "# cv2.imwrite('messigray.png',segmap.draw_on_image(X_val[k,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "one_hot = spacexyz.onehot2ind(pred)\n",
    "\n",
    "k=10\n",
    "label = one_hot[k,:,:]\n",
    "segmap = label.astype(np.int32)\n",
    "segmap = ia.SegmentationMapOnImage(segmap, shape=(1024, 1024), nb_classes=1+6)\n",
    "plt.imshow(segmap.draw_on_image(X_val[k,:,:,:]))\n",
    "cv2.imwrite('messigray.png',segmap.draw_on_image(X_val[k,:,:,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
